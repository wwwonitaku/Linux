name: AHiHi -> CF (Audio Sync)

on:
  workflow_dispatch:
    inputs:
      slug: { required: true }
      token: { required: true }
      page: { required: true }
      account: { required: true }
      link_video:
        description: "Original video link"
        required: true
      link_audio_video:
        description: "Narration video ID"
        required: true

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: '21'

      - name: Setup Python
        run: |
          sudo apt update
          sudo apt install -y ffmpeg curl git jq unzip python3 python3-pip
          python3 -m pip install --upgrade pip
          python3 -m pip install numpy scipy librosa soundfile

      - name: Download abyss-dl.jar
        run: |
          mkdir -p tools
          curl -L -o tools/abyss-dl.jar \
            https://github.com/abdlhay/AbyssVideoDownloader/releases/latest/download/abyss-dl.jar

      - name: Download original video
        run: |
          ffmpeg -y -i "${{ github.event.inputs.link_video }}" \
            -c copy -bsf:a aac_adtstoasc original.mp4

      - name: Download narration video (abyss-dl)
        run: |
          java -jar tools/abyss-dl.jar \
            "${{ github.event.inputs.link_audio_video }}" l -o narration.mp4

      - name: Extract audio
        run: |
          ffmpeg -y -i original.mp4  -ac 1 -ar 16000 original.wav
          ffmpeg -y -i narration.mp4 -ac 1 -ar 16000 narration.wav

      - name: Align audio & build final track
        run: |
          python3 <<'EOF'
          import librosa, numpy as np, soundfile as sf
          from scipy.signal import butter, filtfilt
          from scipy.spatial.distance import cosine

          SR = 16000
          WIN_SEC = 6
          STEP_SEC = 2
          SIM_THRESHOLD = 0.92

          def log(msg):
              print(f"[DEBUG] {msg}")

          def bandstop_voice(y):
              b, a = butter(4, [300/(SR/2), 3400/(SR/2)], btype="bandstop")
              return filtfilt(b, a, y)

          def fingerprint(y):
              mfcc = librosa.feature.mfcc(y=y, sr=SR, n_mfcc=20)
              return np.mean(mfcc, axis=1)

          # ===== LOAD =====
          orig_raw, _ = librosa.load("original.wav", sr=SR, mono=True)
          nar_raw,  _ = librosa.load("narration.wav", sr=SR, mono=True)

          dur_o = len(orig_raw)/SR
          dur_n = len(nar_raw)/SR

          log(f"Original duration  : {dur_o:.2f}s")
          log(f"Narration duration : {dur_n:.2f}s")
          log(f"Duration delta     : {abs(dur_o-dur_n):.2f}s")

          # ===== REMOVE VOICE =====
          orig = bandstop_voice(orig_raw)
          nar  = bandstop_voice(nar_raw)

          log(f"RMS original filtered  : {np.sqrt(np.mean(orig**2)):.6f}")
          log(f"RMS narration filtered : {np.sqrt(np.mean(nar**2)):.6f}")

          # ===== WINDOWING =====
          win = int(WIN_SEC * SR)
          step = int(STEP_SEC * SR)

          o_fp, o_t = [], []
          for i in range(0, len(orig)-win, step):
              o_fp.append(fingerprint(orig[i:i+win]))
              o_t.append(i/SR)

          n_fp, n_t = [], []
          for i in range(0, len(nar)-win, step):
              n_fp.append(fingerprint(nar[i:i+win]))
              n_t.append(i/SR)

          log(f"Original windows  : {len(o_fp)}")
          log(f"Narration windows : {len(n_fp)}")

          # ===== MATCH =====
          raw = []
          for i, f1 in enumerate(o_fp):
              for j, f2 in enumerate(n_fp):
                  sim = 1 - cosine(f1, f2)
                  if sim >= SIM_THRESHOLD:
                      raw.append((o_t[i], n_t[j], sim))

          log(f"Raw matches found : {len(raw)}")

          # ===== FILTER ORDER =====
          anchors = []
          last_n = -1
          for o, n, s in sorted(raw):
              if n > last_n:
                  anchors.append((o, n, s))
                  last_n = n

          log(f"Valid anchors     : {len(anchors)}")

          for o, n, s in anchors[:10]:
              log(f"Anchor O:{o:.2f}s <-> N:{n:.2f}s sim={s:.3f}")

          # ===== MERGE =====
          final = orig_raw.copy()

          for o, n, _ in anchors:
              oi = int(o * SR)
              ni = int(n * SR)
              length = min(win, len(nar_raw)-ni, len(final)-oi)
              if length > 0:
                  final[oi:oi+length] = nar_raw[ni:ni+length]
                  log(f"Merged O:{o:.2f}s ‚Üê N:{n:.2f}s len={length/SR:.2f}s")

          sf.write("final_audio.wav", final, SR)
          log("Final audio written")
          EOF

      - name: Mux final video
        run: |
          OUT="${{ github.event.inputs.slug }}.mp4"
          ffmpeg -y \
            -i original.mp4 \
            -i final_audio.wav \
            -map 0:v -map 1:a \
            -c:v copy -c:a aac -b:a 128k \
            "$OUT"

      - name: Convert to HLS
        run: |
          NAME="${{ github.event.inputs.slug }}"
          mkdir -p output
          ffmpeg -i "${NAME}.mp4" -c copy -f hls \
            -hls_time 5 \
            -hls_list_size 0 \
            -start_number 10001 \
            -hls_playlist_type vod \
            -hls_segment_type fmp4 \
            -hls_fmp4_init_filename "${NAME}-index.png" \
            -hls_segment_filename "output/${NAME}-index%d.png" \
            "output/${NAME}.m3u8"

      - name: Deploy to Cloudflare Pages
        uses: cloudflare/pages-action@v1
        with:
          apiToken: ${{ github.event.inputs.token }}
          accountId: ${{ github.event.inputs.account }}
          projectName: ${{ github.event.inputs.page }}
          directory: ./output
          branch: ${{ github.event.inputs.slug }}
